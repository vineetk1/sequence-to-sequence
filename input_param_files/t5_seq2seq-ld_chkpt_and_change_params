Vineet Kumar, sioom.ai

This input file has user-settable hyper-parameters for loading a model from a checkpoint
   file and further training with different parameters in 'optz_sched'.
Use Case: A user stops training in order to further :w
train with different
    hyper-parameters. Training can be stopped through the keystroke ctrl-c or by 
    using appropriate hyperparameters.

Note the following:
	(1) This file name should be last in the command-line.
	(2) Do NOT change the order of python-dictionaries in this file.
	(3) The default directory is "sequence-tagging"
	(4) All the dictionaries MUST be present even if they are empty
	(5) Unless specified, if a dictionary is empty, its contents will be replaced by the 
	    corresponding dictionary from the checkpoint file that is loaded
	(6) Some dicts -- such as misc, ld_resume_chkpt -- must not be empty

Command-line:
-------------
python3 Main.py input_param_files/bert_seq_tag-ld_chkpt_and_change_params 


parameters for python-dictionary 'misc'. ***Must specify the contents of this dict***
-
{'save_top_k': 2, 'no_testing': False, 'statistics': True}


parameters for python-dictionary 'optz_sched'. ***Must specify the contents of this dict***
- 
{'optz': 'Adam', 'optz_params': {'lr': 2e-7}, 'lr_sched': 'ReduceLROnPlateau', 'lr_sched_params': {'mode': 'min', 'patience': 0, 'factor': 5e-1}} 


parameters for python-dictionary 'data'. 
- 
{}


parameters for python-dictionary 'trainer'. 
- 
{'gpus': 1, 'max_epochs': 1}
#{'gpus': 1, 'max_epochs': 5, 'log_every_n_steps': 100, 'accumulate_grad_batches': 32, 'stochastic_weight_avg': True, 'gradient_clip_val': 0.5}


parameters for python-dictionary 'model_init'. Contents of this dict are replaced by corresponding checkpoint dict 
- 
{}


parameters for python-dictionary 'ld_resume_chkpt'. ***Must specify the contents of this dict***
- 
{'ld_chkpt': 'tensorboard_logs/model=bert,model_type=bert-large-uncased,tokenizer_type=bert/version_5/checkpoints/last.ckpt'}
